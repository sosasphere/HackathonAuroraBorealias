{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15b2d1ac-78ad-4b7f-b4f6-b284fa0a77b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.9.2\n",
      "numpy version: 1.26.4\n",
      "tiktoken version: 0.8.0\n",
      "torch version: 2.5.1+cu118\n",
      "cuda\n",
      "Training loss: 10.986106736319405\n",
      "Validation loss: 11.015321254730225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\AppData\\Local\\Temp\\ipykernel_4024\\20413860.py:216: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 3.561, Val loss 6.516\n",
      "Ep 1 (Step 000005): Train loss 2.355, Val loss 6.537\n",
      "Ep 1 (Step 000010): Train loss 2.404, Val loss 6.406\n",
      "Dynamic Payout is the Account Updater services, businesses to businesses to merchants payment processors, ensuring on recurring revenue.           Account Updater services provide multiple advantages to both businesses of customers, creating a smoother and more\n",
      "Ep 2 (Step 000015): Train loss 1.965, Val loss 6.344\n",
      "Ep 2 (Step 000020): Train loss 1.813, Val loss 6.362\n",
      "Ep 2 (Step 000025): Train loss 1.115, Val loss 6.403\n",
      "Dynamic Payout is the Account Updater services,, and businesses, and even more, and declined transactions.                                \n",
      "Ep 3 (Step 000030): Train loss 0.980, Val loss 6.421\n",
      "Ep 3 (Step 000035): Train loss 0.675, Val loss 6.529\n",
      "Ep 3 (Step 000040): Train loss 0.510, Val loss 6.578\n",
      "Dynamic Payout is a provide insights into customer purchasing habits, and services through their payment systems have evolved tremendously.                                 \n",
      "Ep 4 (Step 000045): Train loss 0.560, Val loss 6.597\n",
      "Ep 4 (Step 000050): Train loss 0.239, Val loss 6.802\n",
      "Ep 4 (Step 000055): Train loss 0.247, Val loss 6.681\n",
      "Dynamic Payout is the Account Updater work flow, which requires several days to complete. For example, if you submit an Account Updater request on Monday (day one), Worldpay accumulates data from the networks Tuesday through Friday (days two through four),\n",
      "Ep 5 (Step 000060): Train loss 0.175, Val loss 6.813\n",
      "Ep 5 (Step 000065): Train loss 0.177, Val loss 6.825\n",
      "Dynamic Payout is a provide insights into customer purchasing habits, seasonal trends, and even regional preferences, allowing businesses to tailor their offerings and marketing efforts. Worldpay’s technology thus goes beyond simply processing payments—it also empowers businesses with data-driven insights.\n",
      "Ep 6 (Step 000070): Train loss 0.131, Val loss 6.907\n",
      "Ep 6 (Step 000075): Train loss 0.104, Val loss 6.961\n",
      "Ep 6 (Step 000080): Train loss 0.131, Val loss 6.984\n",
      "Dynamic Payout is the Account Updater work flow, which requires several days to complete. For example, if you submit an Account Updater request on Monday (day one), Worldpay accumulates data from the networks Tuesday through Friday (days two through four),\n",
      "trainingFiles\\dynamicpayout\\exactpayPayfac.txt\n",
      "Ep 1 (Step 000000): Train loss 4.168, Val loss nan\n",
      "Dynamic Payout is a merchant can. PayF, and, or PayFac, and other. a PayF. Pay-. with a PayF for.   PayF and other PayF PayF for merchants, and a PayF-, or an\n",
      "Ep 2 (Step 000005): Train loss 1.872, Val loss nan\n",
      "Dynamic Payout is a credit card. These innovations enabled individuals and businesses to transact more flexibly and efficiently.   Benefits of the PayFac model in-store, the PayFac model.   The number of SaaS solutions on the market\n",
      "Dynamic Payout is a merchant services. PayFac model. As the transaction details have other non-card payment provider (IS) with a PayFac model.  A payment processing account.    The number of SaaS solutions on the market\n",
      "Ep 4 (Step 000010): Train loss 0.918, Val loss nan\n",
      "Dynamic Payout is payment processing fees. There are many SaaS payment facilitator advantages and SaaS PayFac benefits for small businesses. By operating merchants with a PayFac, SaaS companies and ISOs (IS) with the merchant payment processing fees,\n",
      "Dynamic Payout is convenient online onboarding process. This is a lot easier for merchants than having to fill out extensive paperwork and wait days to weeks to obtain a processing account with a bank.      The number of SaaS solutions on the market\n",
      "Ep 6 (Step 000015): Train loss 0.375, Val loss nan\n",
      "Dynamic Payout is convenient online onboarding process. This is a lot easier for merchants than having to fill out extensive paperwork and wait days to weeks to obtain a processing account with a bank.  Competitive edge The number of SaaS solutions on the market\n",
      "trainingFiles\\dynamicpayout\\payfirmaPaymentProcessing.txt\n",
      "Ep 1 (Step 000000): Train loss 4.896, Val loss 7.065\n",
      "Ep 1 (Step 000005): Train loss 3.085, Val loss 6.485\n",
      "Ep 1 (Step 000010): Train loss 2.180, Val loss 6.367\n",
      "Dynamic Payout is Job - Merchants Job - Card: Visa, and even regional model, and fee is In the merchant account. pay, and the merchant account, and other account. the fee is Interchange for payment Interchange\n",
      "Ep 2 (Step 000015): Train loss 1.971, Val loss 6.396\n",
      "Ep 2 (Step 000020): Train loss 1.354, Val loss 6.447\n",
      "Dynamic Payout is no set industry fee associated with chargebacks. exchange, the merchant to process a credit card and-qualified and) The payment that ensure a safe environment to process credit Chargebacks are transaction where a consumer to\n",
      "Ep 3 (Step 000025): Train loss 1.185, Val loss 6.446\n",
      "Ep 3 (Step 000030): Train loss 0.997, Val loss 6.440\n",
      "Ep 3 (Step 000035): Train loss 0.593, Val loss 6.613\n",
      "Dynamic Payout is no set industry fee associated with chargebacks. Discount Rate (AKA If your business is a merchant account. Rate) The discount rate is a percentage that a merchant must pay to process a transaction where a merchant account\n",
      "Ep 4 (Step 000040): Train loss 0.489, Val loss 6.736\n",
      "Ep 4 (Step 000045): Train loss 0.513, Val loss 6.810\n",
      "Dynamic Payout is no set industry fee associated with chargebacks. Discount Rate (AKA If your business is Merchant Discount Rate) The discount rate is a percentage that a merchant must pay to process a transaction where a qualified (\n",
      "Ep 5 (Step 000050): Train loss 0.331, Val loss 6.782\n",
      "Ep 5 (Step 000055): Train loss 0.286, Val loss 6.687\n",
      "Dynamic Payout is the credit card is not present at the time of purchase, such as online purchases. Credit card data is manually entered instead of swiped. Card Present: Transactions where the credit card is present at the time of purchase, such as\n",
      "Ep 6 (Step 000060): Train loss 0.173, Val loss 6.705\n",
      "Ep 6 (Step 000065): Train loss 0.161, Val loss 6.836\n",
      "Ep 6 (Step 000070): Train loss 0.132, Val loss 6.853\n",
      "Dynamic Payout is no set industry fee associated with chargebacks. Discount Rate (AKA Qualified Rate or Merchant Discount Rate) The discount rate is a percentage that a merchant must pay to process a transaction where a qualified (\n",
      "trainingFiles\\dynamicpayout\\trainingDataDynamicPayout.txt\n",
      "Ep 1 (Step 000000): Train loss 3.745, Val loss 7.343\n",
      "Ep 1 (Step 000005): Train loss 1.903, Val loss 6.841\n",
      "Dynamic Payout is the Worldpay eCommerce and  Worldpay eCommerce  Worldpay e your funds from funds from the Worldpay eCommerce, Worldpay eCommerce funds.  Worldpay eCommerce transactions.  Worldpay eCommerce and to Worldpay eCommerce\n",
      "Ep 2 (Step 000010): Train loss 1.087, Val loss 6.551\n",
      "Dynamic Payout is the same day • Transaction only • Worldpay eComm Dynamic Payout Information • Worldpay eComm Dynamic Payout FAQs • Worldpay eComm Dynamic Payout • Funding Instruction for the same merchant • • World\n",
      "Ep 3 (Step 000015): Train loss 0.627, Val loss 6.662\n",
      "Dynamic Payout is the developer in the order sent. • Worldpay eComm Dynamic Payout Information • Worldpay eComm Dynamic Payout FAQs • Worldpay eComm Dynamic Payout Same Day Funding Instruction Void Dynamic Payout product to settle funds\n",
      "Ep 4 (Step 000020): Train loss 0.324, Val loss 6.670\n",
      "Dynamic Payout is produced either daily or any individual instruction from the Batch that would result in a negative balance. The returned error message provides information about the account lacking funds. For examples of this situation, see Appendix D of the Worldpay cnpAPI Reference\n",
      "Ep 5 (Step 000025): Train loss 0.200, Val loss 6.757\n",
      "Dynamic Payout is Online instruction or any individual instruction from the Batch that would result in a negative balance. The returned error message provides information about the account lacking funds. For examples of this situation, see Appendix D of the Worldpay cnpAPI Reference\n",
      "Ep 6 (Step 000030): Train loss 0.095, Val loss 6.784\n",
      "Ep 6 (Step 000035): Train loss 0.054, Val loss 6.908\n",
      "Dynamic Payout is Online instruction or any individual instruction from the Batch that would result in a negative balance. The returned error message provides information about the account lacking funds. For examples of this situation, see Appendix D of the Worldpay cnpAPI Reference\n",
      "trainingFiles\\common\\AccountUpdater.txt\n",
      "Ep 1 (Step 000000): Train loss 1.133, Val loss 4.425\n",
      "Dynamic Payout is a has become a cornerstone for companies operating in the subscription economy, from streaming services. This also minimizes the need for outreach to customers to the risk of the eCommerce transactions.              \n",
      "Ep 2 (Step 000005): Train loss 0.306, Val loss 3.972\n",
      "Dynamic Payout is a customer has advanced to you in the case of American Express  Account Updater services provider.    Account Updater Services  Account Updater services provide multiple advantages to both businesses and customers, creating a smoother and more\n",
      "Ep 3 (Step 000010): Train loss 0.082, Val loss 4.079\n",
      "Dynamic Payout is a significant.  Additionally, as more businesses shift toward subscription and digital-first models, Account Updater services will become a standard offering. Integration with mobile wallets and digital payment platforms is also expected, ensuring that updated information is available for an\n",
      "Ep 4 (Step 000015): Train loss 0.037, Val loss 4.180\n",
      "Dynamic Payout is a significant.  Additionally, as more businesses shift toward subscription and digital-first models, Account Updater services will become a standard offering. Integration with mobile wallets and digital payment platforms is also expected, ensuring that updated information is available for an\n",
      "Dynamic Payout is a significant.  Additionally, as more businesses shift toward subscription and digital-first models, Account Updater services will become a standard offering. Integration with mobile wallets and digital payment platforms is also expected, ensuring that updated information is available for an\n",
      "Ep 6 (Step 000020): Train loss 0.019, Val loss 4.298\n",
      "Dynamic Payout is a significant By using Account Updater services, businesses can minimize disruptions in service, retain customers, and optimize revenue collection. How Account Updater Services Work  The functionality of Account Updater services involves multiple stages, typically executed through\n",
      "trainingFiles\\common\\Worldpay.txt\n",
      "Ep 1 (Step 000000): Train loss 1.991, Val loss nan\n",
      "Dynamic Payout is a significant.  Additionally, as more businesses shift toward subscription and digital-first models, Account Updater services will become a standard offering. Integration with mobile wallets and digital payment platforms, and digital wallets and customers, creating a smoother and more\n",
      "Dynamic Payout is an American financial technology powerhouse. This merger, valued at around $35 billion, created one of the largest global payment providers and expanded Worldpay’s capabilities, allowing it to serve a broader range of financial services. With this merger, Worldpay\n",
      "Ep 3 (Step 000005): Train loss 0.497, Val loss nan\n",
      "Dynamic Payout is the merchant’s payment systems for instance, which are moved from streaming services. This also minimizes the need for outreach to customers to request updated payment information, allowing it to serve a broader range of financial services. With this merger, Worldpay\n",
      "Dynamic Payout is the merchant’s and that the transaction cycle.    The Technology Behind Worldpay’s account. How Account Updater Services Work  The functionality of Account Updater services are essential for its core, and\n",
      "Dynamic Payout is a leading to the payment in their account, completing the transaction cycle. The Technology Behind Worldpay  One of Worldpay’s greatest strengths is its technology. To ensure the speed, reliability, and security of transactions, Worldpay relies\n",
      "Ep 6 (Step 000010): Train loss 0.121, Val loss nan\n",
      "Dynamic Payout is the merchant receives the payment in their account, completing the transaction cycle. The Technology Behind Worldpay  One of Worldpay’s greatest strengths is its technology. To ensure the speed, reliability, and security of transactions, Worldpay relies\n",
      "Output text:\n",
      " What is Account Updater?\n",
      "\n",
      "In today’s rapidly evolving financial landscape, electronic payment systems are crucial for both consumers and businesses. One of the key players in this domain is Worldpay, a leading payment processing company\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "        \"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")\n",
    "\n",
    "import torch\n",
    "from supplementary import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference\n",
    "\n",
    "import tiktoken\n",
    "from supplementary import generate_text_simple\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "with open(\"trainingDataSmall.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "from supplementary import create_dataloader_v1\n",
    "\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "from supplementary import calc_loss_loader\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "#torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)\n",
    "\n",
    "from supplementary import (\n",
    "    calc_loss_batch,\n",
    "    evaluate_model,\n",
    "    generate_and_print_sample\n",
    ")\n",
    "\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def trainOnFile(model, fileName, gpt_config, tokenizer):\n",
    "    with open(fileName, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_file_data = file.read()\n",
    "\n",
    "    # Train/validation ratio\n",
    "    train_ratio = 0.90\n",
    "    split_idx = int(train_ratio * len(text_file_data))\n",
    "    train_data = text_file_data[:split_idx]\n",
    "    val_data = text_file_data[split_idx:]\n",
    "\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "    train_loader = create_dataloader_v1(\n",
    "        train_data,\n",
    "        batch_size=2,\n",
    "        max_length=gpt_config[\"context_length\"],\n",
    "        stride=gpt_config[\"context_length\"],\n",
    "        drop_last=True,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    val_loader = create_dataloader_v1(\n",
    "        val_data,\n",
    "        batch_size=2,\n",
    "        max_length=gpt_config[\"context_length\"],\n",
    "        stride=gpt_config[\"context_length\"],\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "    num_epochs = 6\n",
    "    train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "        model, train_loader, val_loader, optimizer, device,\n",
    "        num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "        start_context=\"Dynamic Payout is\", tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "# import required module\n",
    "import os\n",
    "\n",
    "def trainOnDirectory(model, directory, gpt_config, tokenizer):\n",
    "    # iterate over files in\n",
    "    # that directory\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(filepath):\n",
    "            print(filepath)\n",
    "            trainOnFile(model, filepath, gpt_config, tokenizer)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# assign directory\n",
    "#trainOnFile(model, 'trainingData.txt', GPT_CONFIG_124M, tokenizer)\n",
    "trainOnFile(model, 'trainingDataSmall.txt', GPT_CONFIG_124M, tokenizer)\n",
    "directory = 'trainingFiles\\dynamicpayout'\n",
    "trainOnDirectory(model, directory, GPT_CONFIG_124M, tokenizer)\n",
    "directory = 'trainingFiles\\common'\n",
    "trainOnDirectory(model, directory, GPT_CONFIG_124M, tokenizer)\n",
    "\n",
    "\n",
    "start_context = \"What is Account Updater?\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer).to(device),\n",
    "    max_new_tokens=40,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779fda84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2128ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "        \"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")\n",
    "\n",
    "import torch\n",
    "from supplementary import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference\n",
    "\n",
    "import tiktoken\n",
    "from supplementary import generate_text_simple\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "with open(\"trainingDataSmall.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "from supplementary import create_dataloader_v1\n",
    "\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "from supplementary import calc_loss_loader\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "#torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)\n",
    "\n",
    "from supplementary import (\n",
    "    calc_loss_batch,\n",
    "    evaluate_model,\n",
    "    generate_and_print_sample\n",
    ")\n",
    "\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def trainOnFile(model, fileName, gpt_config, tokenizer):\n",
    "    with open(fileName, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_file_data = file.read()\n",
    "\n",
    "    # Train/validation ratio\n",
    "    train_ratio = 0.90\n",
    "    split_idx = int(train_ratio * len(text_file_data))\n",
    "    train_data = text_file_data[:split_idx]\n",
    "    val_data = text_file_data[split_idx:]\n",
    "\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "    train_loader = create_dataloader_v1(\n",
    "        train_data,\n",
    "        batch_size=2,\n",
    "        max_length=gpt_config[\"context_length\"],\n",
    "        stride=gpt_config[\"context_length\"],\n",
    "        drop_last=True,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    val_loader = create_dataloader_v1(\n",
    "        val_data,\n",
    "        batch_size=2,\n",
    "        max_length=gpt_config[\"context_length\"],\n",
    "        stride=gpt_config[\"context_length\"],\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "    num_epochs = 6\n",
    "    train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "        model, train_loader, val_loader, optimizer, device,\n",
    "        num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "        start_context=\"Dynamic Payout is\", tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "# import required module\n",
    "import os\n",
    "\n",
    "def trainOnDirectory(model, directory, gpt_config, tokenizer):\n",
    "    # iterate over files in\n",
    "    # that directory\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(filepath):\n",
    "            print(filepath)\n",
    "            trainOnFile(model, filepath, gpt_config, tokenizer)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# assign directory\n",
    "#trainOnFile(model, 'trainingData.txt', GPT_CONFIG_124M, tokenizer)\n",
    "trainOnFile(model, 'trainingDataSmall.txt', GPT_CONFIG_124M, tokenizer)\n",
    "directory = 'trainingFiles\\dynamicpayout'\n",
    "trainOnDirectory(model, directory, GPT_CONFIG_124M, tokenizer)\n",
    "directory = 'trainingFiles\\common'\n",
    "trainOnDirectory(model, directory, GPT_CONFIG_124M, tokenizer)\n",
    "\n",
    "\n",
    "start_context = \"What is Account Updater?\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer).to(device),\n",
    "    max_new_tokens=40,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c77d08eb-427f-4526-814f-f934866d086d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\AppData\\Local\\Temp\\ipykernel_4024\\349987593.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      "   What is Worldpay’s payment in their account, and businesses and financial landscape, and financial, and businesses of all sizes, from small local shops to accept a significant its technology.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "start_context = \"  What is Worldpay\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer).to(device),\n",
    "    max_new_tokens=55,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "feabb7c6-36f7-483d-b2d6-7edfcf4fa4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " What is a FIPC and financial payment in their account, and businesses.\n",
      "\n",
      "\n",
      "The Technology Behind Worldpay\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Worldpay’s services, and payment processing services.\n",
      "\n",
      "\n",
      "Worldpay’s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_context = \"What is a FIPC\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer).to(device),\n",
    "    max_new_tokens=50,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0992f227-a101-4dad-962b-3683c8c0fba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Chargebacks transactions are                             and that the is its services is its of the this, and that and that enables businesses to accept payments through of\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Chargebacks transactions are \"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer).to(device),\n",
    "    max_new_tokens=50,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "545454a3-fd9a-4046-96a5-2360fe90c48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Tell me about Payfac Fees.\n",
      "\n",
      "\n",
      "Worldpay’s range of services is extensive, and reach of payment processing industry, from small local shops to pay a significant its technology, and various options, and payment processing services.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_context = \"Tell me about Payfac Fees.\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer).to(device),\n",
    "    max_new_tokens=50,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7907245-7384-44f3-a2c5-3d909c9b830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "        \"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")\n",
    "\n",
    "import torch\n",
    "from supplementary import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference\n",
    "\n",
    "import tiktoken\n",
    "from supplementary import generate_text_simple\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"A chargeback \"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "with open(\"mediumTraining.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "from supplementary import create_dataloader_v1\n",
    "\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "from supplementary import calc_loss_loader\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)\n",
    "\n",
    "from supplementary import (\n",
    "    calc_loss_batch,\n",
    "    evaluate_model,\n",
    "    generate_and_print_sample\n",
    ")\n",
    "\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")   \n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"A chargeback fee is a\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "torch.save(model, \"model.pth\")\n",
    "\n",
    "start_context = \"What is Account Updater?\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer).to(device),\n",
    "    max_new_tokens=40,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc23f489-4d6e-48ea-9062-59783d659df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      "  What is Account Updater?\n",
      "\n",
      "PERICLES.\n",
      "He thinks not a man.\n",
      "\n",
      "THIRD FISHERMAN.\n",
      "’Tis a goodly.\n",
      "\n",
      "THIRD FISHERMAN.\n",
      "He is a goodly raim.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_context = \" What is Account Updater?\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer).to(device),\n",
    "    max_new_tokens=50,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f74c61b-ec95-4e92-af4d-386a1dd4f55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Tell me more about POS transactions.\n",
      "\n",
      "[_Exit._]\n",
      "\n",
      "SCENE V. The same. The same. A room in the castle\n",
      "\n",
      "Enter Timon and Lance.\n",
      "\n",
      "FIRST FRIEND.\n",
      "I have told you, sir, I have been\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Tell me more about POS transactions\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer).to(device),\n",
    "    max_new_tokens=50,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe9f9734-4943-490e-ad7e-ed62545a2616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Tell me more about POS transactions.\n",
      "\n",
      "[_Exit._]\n",
      "\n",
      "SCENE V. The same. The same. A room in the Garter Inn\n",
      "\n",
      "Enter Host and Simple.\n",
      "\n",
      "HOST.\n",
      "I pray you, sir, I pray you, sir\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Tell me more about POS transactions\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer).to(device),\n",
    "    max_new_tokens=50,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b9e63-72b4-486f-9781-c07d5c6173f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
