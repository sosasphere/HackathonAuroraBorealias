{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15b2d1ac-78ad-4b7f-b4f6-b284fa0a77b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.9.2\n",
      "numpy version: 1.26.3\n",
      "tiktoken version: 0.8.0\n",
      "torch version: 2.5.1+cu118\n",
      "cuda\n",
      "Training loss: 10.976698673132693\n",
      "Validation loss: 10.961809476216635\n",
      "Ep 1 (Step 000000): Train loss 10.048, Val loss 10.134\n",
      "Ep 1 (Step 000005): Train loss 8.449, Val loss 8.719\n",
      "Ep 1 (Step 000010): Train loss 7.551, Val loss 7.801\n",
      "Ep 1 (Step 000015): Train loss 7.068, Val loss 7.424\n",
      "Ep 1 (Step 000020): Train loss 6.804, Val loss 7.396\n",
      "Ep 1 (Step 000025): Train loss 6.740, Val loss 7.253\n",
      "Ep 1 (Step 000030): Train loss 6.613, Val loss 7.099\n",
      "A chargeback fee is a                                                  \n",
      "Ep 2 (Step 000035): Train loss 6.404, Val loss 7.038\n",
      "Ep 2 (Step 000040): Train loss 6.568, Val loss 7.011\n",
      "Ep 2 (Step 000045): Train loss 6.357, Val loss 7.133\n",
      "Ep 2 (Step 000050): Train loss 6.776, Val loss 7.290\n",
      "Ep 2 (Step 000055): Train loss 6.283, Val loss 7.153\n",
      "Ep 2 (Step 000060): Train loss 6.314, Val loss 7.046\n",
      "Ep 2 (Step 000065): Train loss 6.301, Val loss 6.954\n",
      "A chargeback fee is a                                                  \n",
      "Ep 3 (Step 000070): Train loss 6.214, Val loss 6.884\n",
      "Ep 3 (Step 000075): Train loss 6.363, Val loss 6.895\n",
      "Ep 3 (Step 000080): Train loss 6.063, Val loss 6.876\n",
      "Ep 3 (Step 000085): Train loss 6.077, Val loss 6.792\n",
      "Ep 3 (Step 000090): Train loss 6.140, Val loss 6.702\n",
      "Ep 3 (Step 000095): Train loss 5.910, Val loss 6.637\n",
      "A chargeback fee is a      • the • a  • the    • a • the • the merchant   The, and the  • the merchant • the transaction.  • the merchant\n",
      "Ep 4 (Step 000100): Train loss 5.857, Val loss 6.643\n",
      "Ep 4 (Step 000105): Train loss 5.681, Val loss 6.694\n",
      "Ep 4 (Step 000110): Train loss 5.716, Val loss 6.585\n",
      "Ep 4 (Step 000115): Train loss 5.429, Val loss 6.498\n",
      "Ep 4 (Step 000120): Train loss 5.585, Val loss 6.381\n",
      "Ep 4 (Step 000125): Train loss 5.355, Val loss 6.228\n",
      "Ep 4 (Step 000130): Train loss 4.872, Val loss 6.265\n",
      "A chargeback fee is a                                                  \n",
      "Ep 5 (Step 000135): Train loss 4.830, Val loss 6.155\n",
      "Ep 5 (Step 000140): Train loss 4.673, Val loss 6.240\n",
      "Ep 5 (Step 000145): Train loss 4.509, Val loss 6.143\n",
      "Ep 5 (Step 000150): Train loss 4.583, Val loss 6.068\n",
      "Ep 5 (Step 000155): Train loss 4.267, Val loss 6.020\n",
      "Ep 5 (Step 000160): Train loss 4.286, Val loss 5.997\n",
      "A chargeback fee is a  Account Updater Services The PayFac model, and is the transaction. Payment Processing Payment Processing Payment Processing 101 The payment processors, and Payment Processing, and is a Payment Processing are\n",
      "Ep 6 (Step 000165): Train loss 3.980, Val loss 6.029\n",
      "Ep 6 (Step 000170): Train loss 3.810, Val loss 6.078\n",
      "Ep 6 (Step 000175): Train loss 3.905, Val loss 6.103\n",
      "Ep 6 (Step 000180): Train loss 3.510, Val loss 6.043\n",
      "Ep 6 (Step 000185): Train loss 3.618, Val loss 6.018\n",
      "Ep 6 (Step 000190): Train loss 2.932, Val loss 5.997\n",
      "Ep 6 (Step 000195): Train loss 3.243, Val loss 5.978\n",
      "A chargeback fee is a PayFac model, and    The PayFacs can be    •          • Worldpay’t     • Worldpay eComm    \n",
      "Ep 7 (Step 000200): Train loss 2.870, Val loss 5.975\n",
      "Ep 7 (Step 000205): Train loss 2.664, Val loss 5.998\n",
      "Ep 7 (Step 000210): Train loss 2.556, Val loss 5.987\n",
      "Ep 7 (Step 000215): Train loss 2.755, Val loss 5.992\n",
      "Ep 7 (Step 000220): Train loss 2.654, Val loss 5.958\n",
      "Ep 7 (Step 000225): Train loss 2.118, Val loss 5.979\n",
      "Ep 7 (Step 000230): Train loss 2.227, Val loss 5.943\n",
      "A chargeback fee is a necessity. • Worldpay. • Worldpay. • Worldpay eComm Dynamic Payout Current Account Updater services are  • Worldpay eComm Dynamic Payout product or service, and to assist in the cardholder�\n",
      "Ep 8 (Step 000235): Train loss 2.047, Val loss 5.995\n",
      "Ep 8 (Step 000240): Train loss 1.767, Val loss 5.958\n",
      "Ep 8 (Step 000245): Train loss 1.681, Val loss 6.022\n",
      "Ep 8 (Step 000250): Train loss 1.649, Val loss 6.067\n",
      "Ep 8 (Step 000255): Train loss 1.426, Val loss 6.053\n",
      "Ep 8 (Step 000260): Train loss 1.508, Val loss 6.049\n",
      "A chargeback fee is a necessity.  This is the Batch that is the Monthly, you must submit the Batch individually in a Batch file. Payment Processing merchants together and allow them to process payments This is not more than\n",
      "Ep 9 (Step 000265): Train loss 1.361, Val loss 6.108\n",
      "Ep 9 (Step 000270): Train loss 1.271, Val loss 6.097\n",
      "Ep 9 (Step 000275): Train loss 1.368, Val loss 6.188\n",
      "Ep 9 (Step 000280): Train loss 1.225, Val loss 6.151\n",
      "Ep 9 (Step 000285): Train loss 0.972, Val loss 6.096\n",
      "Ep 9 (Step 000290): Train loss 0.936, Val loss 6.162\n",
      "Ep 9 (Step 000295): Train loss 0.904, Val loss 6.188\n",
      "A chargeback fee is a real • • Mid-qualified • Worldpay • Worldpay. • Worldpay e • Worldpay. • Worldpay Dynamic Payout funding model. • Worldpay eComm • Worldpay eComm Dynamic\n",
      "Ep 10 (Step 000300): Train loss 0.869, Val loss 6.291\n",
      "Ep 10 (Step 000305): Train loss 0.664, Val loss 6.245\n",
      "Ep 10 (Step 000310): Train loss 0.814, Val loss 6.278\n",
      "Ep 10 (Step 000315): Train loss 0.682, Val loss 6.353\n",
      "Ep 10 (Step 000320): Train loss 0.505, Val loss 6.342\n",
      "Ep 10 (Step 000325): Train loss 0.548, Val loss 6.326\n",
      "A chargeback fee is a real Guide • Mid-ups. Merchant: A company accepting payment cards in exchange for goods or services. Merchant Account: A type of bank account that allows businesses to accept payments by payment cards, it’s greatest\n",
      "Output text:\n",
      " What is Account Updater?\n",
      "\n",
      "Why Become a PayFac? PayFac Benefits\n",
      "The PayFac model doesn’t only benefit merchants. The merchant’s rapidly evolving financial landscape, debit card companies. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "        \"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")\n",
    "\n",
    "import torch\n",
    "from supplementary import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference\n",
    "\n",
    "import tiktoken\n",
    "from supplementary import generate_text_simple\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "with open(\"trainingDataSmall.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "from supplementary import create_dataloader_v1\n",
    "\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "from supplementary import calc_loss_loader\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)\n",
    "\n",
    "from supplementary import (\n",
    "    calc_loss_batch,\n",
    "    evaluate_model,\n",
    "    generate_and_print_sample\n",
    ")\n",
    "\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"A chargeback fee is a\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "torch.save(model, \"model.pth\")\n",
    "\n",
    "start_context = \"What is Account Updater?\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer).to(device),\n",
    "    max_new_tokens=40,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feabb7c6-36f7-483d-b2d6-7edfcf4fa4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " What is Account Updater?\n",
      "The functionality of Account Updater services involves multiple stages, typically executed through partnerships with card networks like Visa and Mastercard, which provide regular updates on changes to cardholder information.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_context = \"What is Account Updater?\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer).to(device),\n",
    "    max_new_tokens=40,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7907245-7384-44f3-a2c5-3d909c9b830e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
